<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Strict//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>/Users/heinz/git/solace-kafka-connector-sink/.README.md.html</title>


<style type="text/css">
body {
	color: #333;
	font: 13px/1.4 "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
	padding: 0;
	margin: 0;
}

a {
	background: transparent;
	color: #4183c4;
	text-decoration: none;
}

a:active,
a:hover {
	outline: 0 none;
	text-decoration: underline;
}

abbr[title] {
	border-bottom: 1px dotted;
}

b,
strong {
	font-weight: bold;
}

dfn {
	font-style: italic;
}
h1 {
	font-size: 2em;
	margin: 0.67em 0;
}
mark {
	background: #ff0;
	color: #000;
}
small {
	font-size: 80%;
}
sub, sup {
	font-size: 75%;
	line-height: 0;
	position: relative;
	vertical-align: baseline;
}
sup {
	top: -0.5em;
}
sub {
	bottom: -0.25em;
}
img {
	border: 0 none;
}
svg:not(:root) {
	overflow: hidden;
}
figure {
	margin: 1em 40px;
}
hr {
	box-sizing: content-box;
	height: 0;
}

code,
kbd,
pre,
samp {
	font-family: monospace,monospace;
	font-size: 1em;
}

pre {
	overflow: auto;
	font: 12px Consolas,"Liberation Mono",Menlo,Courier,monospace;
	margin-bottom: 0;
	margin-top: 0;
}

.markdown-body {
	padding: 30px;
	font-size: 16px;
	line-height: 1.6;
	word-wrap: break-word;
}

.markdown-body>*:first-child {
	margin-top: 0 !important;
}

.markdown-body>*:last-child {
	margin-bottom: 0 !important;
}

.markdown-body .absent {
	color: #c00;
}

.markdown-body .anchor {
	position: absolute;
	top: 0;
	bottom: 0;
	left: 0;
	display: block;
	padding-right: 6px;
	padding-left: 30px;
	margin-left: -30px;
}

.markdown-body .anchor:focus {
	outline: none;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
	position: relative;
	margin-top: 1em;
	margin-bottom: 16px;
	font-weight: bold;
	line-height: 1.4;
}

.markdown-body h1 .octicon-link,
.markdown-body h2 .octicon-link,
.markdown-body h3 .octicon-link,
.markdown-body h4 .octicon-link,
.markdown-body h5 .octicon-link,
.markdown-body h6 .octicon-link {
	display: none;
	color: #000;
	vertical-align: middle;
}

.markdown-body h1:hover .anchor,
.markdown-body h2:hover .anchor,
.markdown-body h3:hover .anchor,
.markdown-body h4:hover .anchor,
.markdown-body h5:hover .anchor,
.markdown-body h6:hover .anchor {
	padding-left: 8px;
	margin-left: -30px;
	line-height: 1;
	text-decoration: none;
}

.markdown-body h1:hover .anchor .octicon-link,
.markdown-body h2:hover .anchor .octicon-link,
.markdown-body h3:hover .anchor .octicon-link,
.markdown-body h4:hover .anchor .octicon-link,
.markdown-body h5:hover .anchor .octicon-link,
.markdown-body h6:hover .anchor .octicon-link {
	display: inline-block;
}

.markdown-body h1 tt,
.markdown-body h1 code,
.markdown-body h2 tt,
.markdown-body h2 code,
.markdown-body h3 tt,
.markdown-body h3 code,
.markdown-body h4 tt,
.markdown-body h4 code,
.markdown-body h5 tt,
.markdown-body h5 code,
.markdown-body h6 tt,
.markdown-body h6 code {
	font-size: inherit;
}

.markdown-body h1 {
	padding-bottom: 0.3em;
	font-size: 2.25em;
	line-height: 1.2;
	border-bottom: 1px solid #eee;
}

.markdown-body h2 {
	padding-bottom: 0.3em;
	font-size: 1.75em;
	line-height: 1.225;
	border-bottom: 1px solid #eee;
}

.markdown-body h3 {
	font-size: 1.5em;
	line-height: 1.43;
}

.markdown-body h4 {
	font-size: 1.25em;
}

.markdown-body h5 {
	font-size: 1em;
}

.markdown-body h6 {
	font-size: 1em;
	color: #777;
}

.markdown-body p,.markdown-body blockquote,
.markdown-body ul,.markdown-body ol,
.markdown-body dl,.markdown-body table,
.markdown-body pre {
	margin-top: 0;
	margin-bottom: 16px;
}

.markdown-body hr {
	height: 4px;
	padding: 0;
	margin: 16px 0;
	background-color: #e7e7e7;
	border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
	padding-left: 2em;
}

.markdown-body ul.no-list,
.markdown-body ol.no-list {
	padding: 0;
	list-style-type: none;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
	margin-top: 0;
	margin-bottom: 0;
}

.markdown-body li>p {
	margin-top: 16px;
}

.markdown-body dl {
	padding: 0;
}

.markdown-body dl dt {
	padding: 0;
	margin-top: 16px;
	font-size: 1em;
	font-style: italic;
	font-weight: bold;
}

.markdown-body dl dd {
	padding: 0 16px;
	margin-bottom: 16px;
}

.markdown-body blockquote {
	padding: 0 15px;
	color: #777;
	border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
	margin-top: 0;
}

.markdown-body blockquote>:last-child {
	margin-bottom: 0;
}

.markdown-body table {
	display: block;
	width: 100%;
	overflow: auto;
	word-break: normal;
	word-break: keep-all;
}

.markdown-body table th {
	font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
	padding: 6px 13px;
	border: 1px solid #ddd;
}

.markdown-body table tr {
	background-color: #fff;
	border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
	background-color: #f8f8f8;
}

.markdown-body img {
	max-width: 100%;
	-moz-box-sizing: border-box;
	box-sizing: border-box;
}

.markdown-body span.frame {
	display: block;
	overflow: hidden;
}

.markdown-body span.frame>span {
	display: block;
	float: left;
	width: auto;
	padding: 7px;
	margin: 13px 0 0;
	overflow: hidden;
	border: 1px solid #ddd;
}

.markdown-body span.frame span img {
	display: block;
	float: left;
}

.markdown-body span.frame span span {
	display: block;
	padding: 5px 0 0;
	clear: both;
	color: #333;
}

.markdown-body span.align-center {
	display: block;
	overflow: hidden;
	clear: both;
}

.markdown-body span.align-center>span {
	display: block;
	margin: 13px auto 0;
	overflow: hidden;
	text-align: center;
}

.markdown-body span.align-center span img {
	margin: 0 auto;
	text-align: center;
}

.markdown-body span.align-right {
	display: block;
	overflow: hidden;
	clear: both;
}

.markdown-body span.align-right>span {
	display: block;
	margin: 13px 0 0;
	overflow: hidden;
	text-align: right;
}

.markdown-body span.align-right span img {
	margin: 0;
	text-align: right;
}

.markdown-body span.float-left {
	display: block;
	float: left;
	margin-right: 13px;
	overflow: hidden;
}

.markdown-body span.float-left span {
	margin: 13px 0 0;
}

.markdown-body span.float-right {
	display: block;
	float: right;
	margin-left: 13px;
	overflow: hidden;
}

.markdown-body span.float-right>span {
	display: block;
	margin: 13px auto 0;
	overflow: hidden;
	text-align: right;
}

.markdown-body code,.markdown-body tt {
	padding: 0;
	padding-top: 0.2em;
	padding-bottom: 0.2em;
	margin: 0;
	font-size: 85%;
	background-color: rgba(0,0,0,0.04);
	border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after,
.markdown-body tt:before,
.markdown-body tt:after {
	letter-spacing: -0.2em;
	content: "\00a0";
}

.markdown-body code br,
.markdown-body tt br {
	display: none;
}

.markdown-body del code {
	text-decoration: inherit;
}

.markdown-body pre>code {
	padding: 0;
	margin: 0;
	font-size: 100%;
	word-break: normal;
	white-space: pre;
	background: transparent;
	border: 0;
}

.markdown-body .highlight {
	margin-bottom: 16px;
}

.markdown-body .highlight pre,
.markdown-body pre {
	padding: 16px;
	overflow: auto;
	font-size: 85%;
	line-height: 1.45;
	background-color: #f7f7f7;
	border-radius: 3px;
}

.markdown-body .highlight pre {
	margin-bottom: 0;
	word-break: normal;
}

.markdown-body pre {
	word-wrap: normal;
}

.markdown-body pre code,
.markdown-body pre tt {
	display: inline;
	max-width: initial;
	padding: 0;
	margin: 0;
	overflow: initial;
	line-height: inherit;
	word-wrap: normal;
	background-color: transparent;
	border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after,
.markdown-body pre tt:before,
.markdown-body pre tt:after {
	content: normal;
}

.highlight .pl-coc,
.highlight .pl-entl,
.highlight .pl-entm,
.highlight .pl-eoa,
.highlight .pl-mai .pl-sf,
.highlight .pl-mm,
.highlight .pl-pdv,
.highlight .pl-sc,
.highlight .pl-som,
.highlight .pl-sr,
.highlight .pl-v,
.highlight .pl-vpf {
	color: #0086b3;
}
.highlight .pl-eoac,
.highlight .pl-mdht,
.highlight .pl-mi1,
.highlight .pl-mri,
.highlight .pl-va,
.highlight .pl-vpu {
	color: #008080;
}
.highlight .pl-c,
.highlight .pl-pdc {
	color: #b4b7b4;
	font-style: italic;
}
.highlight .pl-k,
.highlight .pl-ko,
.highlight .pl-kolp,
.highlight .pl-mc,
.highlight .pl-mr,
.highlight .pl-ms,
.highlight .pl-s,
.highlight .pl-sok,
.highlight .pl-st {
	color: #6e5494;
}
.highlight .pl-ef,
.highlight .pl-enf,
.highlight .pl-enm,
.highlight .pl-entc,
.highlight .pl-eoi,
.highlight .pl-sf,
.highlight .pl-smc {
	color: #d12089;
}
.highlight .pl-ens,
.highlight .pl-eoai,
.highlight .pl-kos,
.highlight .pl-mh .pl-pdh,
.highlight .pl-mp,
.highlight .pl-pde,
.highlight .pl-stp {
	color: #458;
}
.highlight .pl-enti {
	color: #d12089;
	font-weight: bold;
}
.highlight .pl-cce,
.highlight .pl-enc,
.highlight .pl-kou,
.highlight .pl-mq {
	color: #f93;
}
.highlight .pl-mp1 .pl-sf {
	color: #458;
	font-weight: bold;
}
.highlight .pl-cos,
.highlight .pl-ent,
.highlight .pl-md,
.highlight .pl-mdhf,
.highlight .pl-ml,
.highlight .pl-pdc1,
.highlight .pl-pds,
.highlight .pl-s1,
.highlight .pl-scp,
.highlight .pl-sol {
	color: #df5000;
}
.highlight .pl-c1,
.highlight .pl-cn,
.highlight .pl-pse,
.highlight .pl-pse .pl-s2,
.highlight .pl-vi {
	color: #a31515;
}
.highlight .pl-mb,
.highlight .pl-pdb {
	color: #df5000;
	font-weight: bold;
}
.highlight .pl-mi,
.highlight .pl-pdi {
	color: #6e5494;
	font-style: italic;
}
.highlight .pl-ms1 {
	background-color: #f5f5f5;
}
.highlight .pl-mdh,
.highlight .pl-mdi {
	font-weight: bold;
}
.highlight .pl-mdr {
	color: #0086b3;
	font-weight: bold;
}
.highlight .pl-s2 {
	color: #333;
}
.highlight .pl-ii {
	background-color: #df5000;
	color: #fff;
}
.highlight .pl-ib {
	background-color: #f93;
}
.highlight .pl-id {
	background-color: #a31515;
	color: #fff;
}
.highlight .pl-iu {
	background-color: #b4b7b4;
}
.highlight .pl-mo {
	color: #969896;
}

</style>


<script type="text/javascript">

function getDocumentScrollTop() 
{
   var res = document.body.scrollTop || document.documentElement.scrollTop || window.pageYOffset || 0;
   // alert(res);
   return res;
}

function setDocumentScrollTop(ypos) 
{
	window.scrollTo(0, ypos);
}

</script>


</head>
<body class="markdown-body">
<p><a href="https://travis-ci.org/SolaceLabs/solace-kafka-connector" rel="nofollow"><img src="https://camo.githubusercontent.com/48e49ffbd3681027336d9944ee0a9b5c7cf3feba/68747470733a2f2f7472617669732d63692e6f72672f536f6c6163654c6162732f736f6c6163652d6b61666b612d636f6e6e6563746f722d73696e6b2e7376673f6272616e63683d646576656c6f706d656e74" alt="Build Status" data-canonical-src="https://travis-ci.org/SolaceLabs/solace-kafka-connector-sink.svg?branch=development" style="max-width:100%;" /></a></p> 
<h1> <a id="solace-sink-kafka-connector-v10" class="anchor" href="#solace-sink-kafka-connector-v10" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Solace Sink Kafka Connector v1.0</h1> 
<h2> <a id="synopsis" class="anchor" href="#synopsis" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Synopsis</h2> 
<p>This project provides a Solace/Kafka Sink Connector (adapter) that makes use of the Kafka Connect libraries. The Solace/Kafka adapter consumes Kafka topic records and streams the data events to the Solace Event Mesh as a Topic and/or Queue data event.</p> 
<p>On the Solace side of the Sink Connector the adapter is using Solace's high performance Java API to stream Solace messages to a Solace Broker (PubSub+ appliance, software or Solace Cloud service). Unlike many other message brokers, Solace supports transparent protocol and API messaging transformations. Therefore, any message that reaches the Solace broker is not limited to being consumed from the Solace broker only by Java clients using the same JCSMP libraries that were used to send the messages to the Solace Broker. Solace supports transparent interoperability with many message transports and languages/APIs. Therefore, from the single Solace Sink Connector any Kafka Topic (Key or not Keyed) Sink Record is instantly available for consumption by any consumer that uses one of the Solace supported open standards languages or transport protocols.</p> 
<p>Consider the following diagram:</p> 
<p><a href="resources/KSink3.png" target="_blank" rel="noopener noreferrer"><img src="resources/KSink3.png" alt="Architecture Overview" style="max-width:100%;" /></a></p> 
<p>It does not matter that the Kakfa record was consumed by the Connector and sent using Java JCSMP transport to a Solace broker (appliance, software or cloud). The Solace event message can transparently be consumed by a Cell Phone, a REST Server or an AMQP, JMS, MQTT message, etc. as a real-time asynchronous data event.</p> 
<p>The Solace Sink Connector also ties Kafka records into the Solace Event Mesh. The Event Mesh is a clustered group of Solace PubSub+ Brokers that transparently, in real-time, route data events to any Service that is part of the Event Mesh. Solace PubSub+ Brokers (Appliances, Software and SolaceCloud) are connected to each other as a multi-connected mesh that to individual services (consumers or producers of data events) appears to be a single Event Broker. Events messages are seamlessly transported within the entire Solace Event Mesh regardless of where the event is created and where the process exists that has registered interested in consuming the event. Simply by registering interest in receiving events, the entire Event Mesh becomes aware of the registration request and will know how to securely route the appropriate events generated by the Solace Sink Connector.</p> 
<p>The Solace Sink Connector allows the creation and storage of a new Kafka record to become an event in the Solace Event Mesh. The Solace Sink Connector provides the ability to transparently push any new Kafka Record that is placed onto a Kafka Topic into the Solace Event Mesh. That new event can be consumed by any other service that is connected to the Solace Event Mesh and has registered interest in the event. As a result, all other service that are part of the Event Mesh will be able to receive the Kafka Records through this single Solace Sink Connector. There is no longer a requirement for separate Kakfa Sink Connectors to each of the separate services. The single Solace Sink Connector is all that is required. Once the Record is in the Event Mesh, it is available to all other services.</p> 
<p>The Solace Sink Connector eliminates the complexity and overhead of maintaining separate Sink Connectors for each and every service that may be interested in the same data that is placed into a Kafka Topic. There is the added benefit of access to services where there is no Kafka Sink Connector available, thereby eliminating the need to create and maintain a new connector for new services that may be interested in Kafka Records.</p> 
<p>Consider the following:</p> 
<p><a href="resources/EventMesh.png" target="_blank" rel="noopener noreferrer"><img src="resources/EventMesh.png" alt="Event Mesh" style="max-width:100%;" /></a></p> 
<p>A single Solace Sink Connector will be able to move the new Kakfa Record to any downstream service via a single connector.</p> 
<p>The Solace Sink Connector also ties into Solace's location transparency for the Event Mesh PubSub+ brokers. Solace supports a wide range of brokers for deployment. There are three major categories of Solace PubSub+ brokers: dedicated extreme performance hardware appliances, high performance software brokers that are deployed as software images (deployable under most Hypervisors, Cloud IaaS and PaaS layers and in Docker) and provided as a fully managed Cloud MaaS (Messaging as a Service).</p> 
<p>It does not matter what Solace Broker is used or where it is deployed, it can become part of the Solace Event Mesh. Therefore, there are no restrictions on where the Solace Sink Connector is deployed or what PubSub+ broker is used to connect Kafka to the Solace Event Bus. The Solace Event Mesh infrastructure will allow, via the Solace Sink Connector, Kafka events to be consumed by any Service anywhere that is part of the Event Mesh.</p> 
<p>Consider the following:</p> 
<p><a href="resources/SolaceCloud2.png" target="_blank" rel="noopener noreferrer"><img src="resources/SolaceCloud2.png" alt="Location Independence" style="max-width:100%;" /></a></p> 
<p>It does not matter if the Kakfa record storage event was generated by a Solace Sink Connector in the Cloud or on premise. It does not matter if Solace Sink Connector was connected to a Solace PubSub+ broker that was an appliance, on premise or Cloud software, or the the Cloud managed MaaS, it will immediately in real time be available to all Solace Event Mesh connected services that are located anywhere.</p> 
<p>It is important to mention that there is also a Solace Source Connector for Kafka available. The Solace Source Connector allows registration of interest in specific events on the Solace Event Mesh. When these events of interest are consumed by the Solace Source Connector, they are placed as a Kakfa Record onto a Kafka Topic. These events that are stored in Kafka are now transparently available to any application that is consuming Kafka records directly from the Kafka brokers. Please refer to the Solace Source Connector GitHub repository for more details.</p> 
<h2> <a id="usage" class="anchor" href="#usage" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h2> 
<p>This is a Gradle project that references all the required dependencies. To check the code style and find bugs you can use:</p> 
<pre><code>./gradlew clean check
</code></pre> 
<p>To actually create the Connector Jar file use:</p> 
<pre><code>./gradlew clean jar
</code></pre> 
<h2> <a id="deployment" class="anchor" href="#deployment" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Deployment</h2> 
<p>The Solace Sink Connector has been tested in three environments: Apache Kafka, Confluent Kafka and the AWS Confluent Platform. For testing, it is recommended to use the single node deployment of Apache or Confluent Kafka software.</p> 
<p>To deploy the Connector, as described in the Kafka documentation, it is necessary to move the Connector jar file and the required third party jar files to a directory that is part of the Worker-defined classpath. Details for installing the Solace Sink Connector are described in the next two sub sections.</p> 
<h4> <a id="apache-kafka" class="anchor" href="#apache-kafka" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Apache Kafka</h4> 
<p>For Apache Kafka, the software is typically found, for example for the 2.11 version, under the root directory: &quot;/opt/kafka-apache/&quot;kafka_2.11-1.1.0&quot;. Typically the Solace Sink Connector would be placed under the &quot;libs&quot; directory under the root directory. All required Solace JCSMP JAR files should be placed under the same &quot;libs&quot; directory. The properties file for the connector would typically be placed under the &quot;config&quot; directory below the root directory.</p> 
<p>To start the connector in stand-alone mode while in the &quot;bin&quot; directory the command would be similar to:</p> 
<pre><code>./connect-standalone.sh ../config/connect-standalone.properties ../config/solaceSink.properties
</code></pre> 
<p>In this case &quot;solaceSink.properties&quot; is the configuration file that you created to define the connectors behavior. Please refer to the sample included in this project.</p> 
<p>When the connector starts in stand-alone mode, all output goes to the console. If there are errors they should be visible on the console. If you do not want the output to console, simply add the &quot;-daemon&quot; option and all output will be directed to the logs directory.</p> 
<h4> <a id="confluent-kafka" class="anchor" href="#confluent-kafka" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Confluent Kafka</h4> 
<p>The Confluent Kakfa software is typically placed under the root directory: &quot;/opt/confluent/confluent-4.1.1&quot;. In this case it is for the 4.1.1 version of Confluent. By default, the Confluent software is started in distributed mode with the REST Gateway started.</p> 
<p>THe Solace Sink Connector would typically be placed in the &quot;/opt/confluent/confluent-4.1.1/share/java/kafka-connect-solace&quot;. You will need to create the &quot;kafka-connect-solace&quot; directory. You must place all the required Solace JCSMP JAR files under this same directory. If you plan to run the Sink Connector in stand-alone mode, it is suggested to place the properties file under the same directory.</p> 
<p>After the Solace files are installed and if you are familiar with Kakfa administration, it is recommended to restart the Confluent Connect software if Confluent is running in Distributed mode. Alternatively, it is simpler to just start and restart the Confluent software with the &quot;confluent&quot; command.</p> 
<p>At this point you can test to confirm the Solace Sink Connector is available for use in distributed mode with the command:</p> 
<pre><code>curl http://18.218.82.209:8083/connector-plugins | jq
</code></pre> 
<p>In this case the IP address is one of the nodes running the Distributed mode Worker process. If the Connector is loaded correctly, you should see something similar to:</p> 
<p><a href="resources/RESTConnectorListSmall.png" target="_blank" rel="noopener noreferrer"><img src="resources/RESTConnectorListSmall.png" alt="Connector List" style="max-width:100%;" /></a></p> 
<p>At this point, it is now possible to start the connector in distributed mode with a command similar to:</p> 
<pre><code>curl -X POST -H &quot;Content-Type: application/json&quot; -d @solace_sink_properties.json http://18.218.82.209:8083/connectors
</code></pre> 
<p>Again, the IP address is one of the nodes running the Distributed mode Worker process. The connector's JSON configuration file, in this case, is called &quot;solace_sink_properties.json&quot;.</p> 
<p>You can determine if the Sink Connector is running with the following command:</p> 
<div class="highlight highlight-source-ini">
 <pre>curl 18.218.82.209:8083/connectors/solaceSinkConnector/status | jq</pre>
</div> 
<p>If there was an error in starting, the details will be returned with this command. If the Sink Connector was successfully started the status of the connector and task processes will be &quot;running&quot;:</p> 
<p><a href="resources/RESTStatusSmall.png" target="_blank" rel="noopener noreferrer"><img src="resources/RESTStatusSmall.png" alt="Connector Status" style="max-width:100%;" /></a></p> 
<h2> <a id="configuration" class="anchor" href="#configuration" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Configuration</h2> 
<p>The Solace Sink Connector configuration is managed by the configuration file. For stand-alone Kafka deployments a properties file is used. A sample is enclosed with the project.</p> 
<p>For distributed Kafka deployments the connector can be deployed via REST as a JSON configuration file. A sample is enclosed with the project.</p> 
<h4> <a id="solace-configuration-for-the-sink-connector" class="anchor" href="#solace-configuration-for-the-sink-connector" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Solace Configuration for the Sink Connector</h4> 
<p>The Solace configuration of the connector's Solace Session, Transport and Security properties are all available and defined in the <strong>SolaceSinkConstants.java</strong> file. These are the equivalent to the details for the Solace <strong>JCSMPSessionProperties</strong> class. Details and documentation for this JCSMPProperies class can be found here:</p> 
<p><a href="https://docs.solace.com/API-Developer-Online-Ref-Documentation/java/index.html" rel="nofollow">Solace Java API</a></p> 
<p>For tuning, performance and scaling (multiple tasks is supported with this connector) of the Solace Sink Connector, please refer to the Solace PubSub+ documentation that can be found here:</p> 
<p><a href="https://docs.solace.com/" rel="nofollow">Solace PubSub+ Documentation</a></p> 
<p>There is a bare minimum requirement to configure access to the Solace PubSub+ broker. A username, their password and VPN (Solace Virtual Private Network - a &quot;virtual broker&quot; used in Solace multi-tenancy configurations) and host reference are mandatory configuration details. An example of the required configuration file entries is as follows:</p> 
<div class="highlight highlight-source-ini">
 <pre><span class="pl-k">sol.username</span>=user1
<span class="pl-k">sol.password</span>=password1
<span class="pl-k">sol.vpn_name</span>=kafkavpn
<span class="pl-k">sol.host</span>=160.101.136.33</pre>
</div> 
<p>If you have just installed a Solace PubSub+ broker and you are not that familiar with Solace administration, you can test your Sink Connector by using &quot;default&quot; as value for the username, password and VPN name. The host should match the IP address of the broker.</p> 
<p>For connectivity to Kafka, the Sink Connector has four basic configuration requirements: name for the Connector Plugin, the name of the Java Class for the connector, the number of Tasks the connector should deploy and the name of the Kakfa Topic. The following is an example for the Solace Source Connector:</p> 
<div class="highlight highlight-source-ini">
 <pre><span class="pl-k">name</span>=solaceSinkConnector
<span class="pl-k">connector.class</span>=com.solace.sink.connector.SolaceSinkConnector
<span class="pl-k">tasks.max</span>=1
<span class="pl-k">topics</span>=solacetest</pre>
</div> 
<p>A more details example is included with this project. This project also includes a JSON configuration file.</p> 
<h4> <a id="solace-record-processor" class="anchor" href="#solace-record-processor" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Solace Record Processor</h4> 
<p>The processing of the Kafka Source Record to create a Solace message is handled by an interface definition defined in <strong>SolaceRecordProcessor.java</strong> - This is a simple interface that is used to create the Solace event message from the Kafka record. There are three examples included of classes that implement this interface:</p> 
<ul> 
 <li> <strong>SolSimpleRecordProcessor.java</strong> - Takes the Kafka Sink record as a binary payload with a Binary Schema for the value ( which becomes the Solace message payload) and a Binary Schema for the record key and creates and sends the appropriate Solace event message. The Kafka Sink Record key and value scheme can be changed via the configuration file.</li> 
 <li> <strong>SolSimpleKeyedRecordProcessor</strong> - A more complex sample that allows the flexibility of changing the Source Record Key Schema and which value from the Solace message to use as a key. The option of no key in the record is also possible. The Kafka Key is also used as a Correlation ID in the Solace messages and the original Kafka Topic is included for reference in the Solace Message as UserData in the Solace message header.</li> 
 <li> <strong>SolSimpleKeyedRecordProcessorDTO</strong> - This is the same as the &quot;SolSimpleKeyedRecordProcessor&quot;, but it adds a DTO (Deliver-to-One) flag to the Solace Event message. The DTO flag is part of topic consumer scaling. For more details refer to the Solace documentation. The Solace Source Connector also support consuming Solace Event Messages with the DTO flag. More details are available in in GitHub where you will find the Solace Source Task code and details.</li> 
</ul> 
<p>The desired message processor is loaded at runtime based on the configuration of the JSON or properties configuration file, for example:</p> 
<div class="highlight highlight-source-ini">
 <pre><span class="pl-k">sol.record_processor_class</span>=com.solace.sink.connector.recordprocessor.SolSimpleKeyedRecordProcessor</pre>
</div> 
<p>It is possible to create more custom Record Processors based on your Kafka record requirements for keying and/or value serialization and the desired format of the Solace event message. Simply add the new record processor classes to the project. The desired record processor is installed at run time based on the configuration file.</p> 
<p>More information on Kakfa Connect can be found here:</p> 
<p><a href="https://kafka.apache.org/documentation/" rel="nofollow">Apache Kafka Connect</a></p> 
<p><a href="https://docs.confluent.io/current/connect/index.html" rel="nofollow">Confluent Kafka Connect</a></p> 
<h4> <a id="scaling-the-sink-connector" class="anchor" href="#scaling-the-sink-connector" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Scaling the Sink Connector</h4> 
<p>The Solace Sink Connector will scale when more performance is required. There is only so much throughput that can be pushed through the Connect API. The Solace broker supports far greater throughput than can be afforted through a single instance of the Connect API. The Kafka Broker can also produce records at a rate far greater than available through a single instance of the Connector. Therefore, multiple instances of the Sink Connector will increase throughput from the Kafka broker to the Solace PubSub+ broker.</p> 
<p>Multiple Connector tasks are automatically deployed and spread across all available Connect Workers simply by indicating the number of desired tasks in the connector configuration file.</p> 
<p>When the Sink Connector is consuming from Kafka and the event records are expected to be placed in to a Solace Queue, there are no special requirements for the Solace Queue definition. As more instance of the connector are defined in the configuration, they will each simultaneously push event messages into the defined queue.</p> 
<p>The Solace Sink Connector can also be configured to generate Solace Topic event messages when new records are placed into Kafka. There is no special setup on the Solace Broker that the multiple scaled connector instances to scale the performance of Solace topic-based event messages.</p> 
<p>When a Solace Sink Connector is scaled, it will automatically use a Kakfa Consumer Group to allow Kafka to move the records for the multiple Topic Partitions in parallel.</p> 
<h4> <a id="sending-solace-event-messages" class="anchor" href="#sending-solace-event-messages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Sending Solace Event Messages</h4> 
<p>The Kafka Connect API automatically keeps track of the offset that the Sink Connector has read and processed. If the Sink Connector stops or is restarted, the Connect API will start passing records to the Sink Connector based on the last saved offset. Generally, the offset is saved on a timed basis every 10 seconds. This is tunable in the Connect Worker configuration file.</p> 
<p>When the Solace Sink Connector is sending Solace Topic message data events, the chances of duplication and message loss will mimic the underlying reliability and QoS configured for the Kakfa Topic and is also controlled by the timer for flushing the offset value to disk.</p> 
<p>It is also possible to send Kafka Topic messages to Solace queues. A Solace Queue guarantees order of deliver, provides High Availability and Disaster Recovery (depending on the setup of the PubSub+ brokers) and provides an acknowledgment to the message producer (in this case the Solace Sink Connector) when the event is stored in all HA and DR members and flushed to disk. This is a higher guarantee than is provided by Kakfa even for Kafka idempotent delivery.</p> 
<p>When the Solace Sink Connector is sending data events to Queues, the messages are send using a Session Transaction. When 200 events are processed, the Solace Connector automatically forces a flush of the offset and then commits the Solace transactions. If the timer goes off before the 200 messages are send the same flush/commit is executed. Therefore, there should be no duplicates sent to the Service Mesh. However, data loss is a factor of the Kafka Topic's reliability and QoS configuration.</p> 
<p>If there is any error or failure and the Offset location is not synced, the Solace transaction will roll back messages in the queue up until the last offset flush. After the connector is restarted, processing will begin again from the last stored Offset.</p> 
<p>It is recommended to use Solace Topics when sending events if high throughput is required and the Kakfa Topic is configured for high performance. When a Kakfa topic is configured for it's highest throughput it will potentially result in loss or duplication within the processing of records in the Kafka Topic.</p> 
<p>Increasing the reliability of the Kafka Topic processing to reduce the potential loss or duplication, but will also greatly reduce throughput. When Kafka reliability is critical, it may be recommended to mimic this reliability with the Solace Sink Connector and configure the connector to send the Kafka records to the Event Mesh using Solace Queues.</p> 
<h4> <a id="dynamic-destinations" class="anchor" href="#dynamic-destinations" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dynamic Destinations</h4> 
<p>By default, the Sink Connector will send messages from the Kafka Records to the Destinations (Topic or Queues) defined in the configuration file (Properties or JSON file). In some cases, it may be desirable to send each Kafka Record to a different Solace Topic based on the details in the Kafka Record. This would mean that rather than using the static Solace Topic defined in the configuration file, a dynamic Solace Topic would need to be created for each record.</p> 
<p>Generally, a Solace Topic is a hierarchical meta-data representation that describes the message payload. Therefore, it is generally possible to form a Solace Topic that matches the &quot;rules&quot; defined to generate a topic from the data in the payload. In this way each Kafka Record from the same Kafka Topic could be targeted to a potentially different Solace Topic.</p> 
<p>To make use of dynamic topics in the Solace Record Processors, it is necessary to update the configuration to indicate to the Solace Sink Connector to ignore the configuration destination references with the following entry:</p> 
<div class="highlight highlight-source-ini">
 <pre><span class="pl-k">sol.dynamic_destination</span>=true</pre>
</div> 
<p>This entry in the configuration indicates that the actual destination must be defined in the record processor. To add the dynamic Solace Topic in the record processor, it necessary to add the details into the user defined Solace Header, for example:</p> 
<div class="highlight highlight-source-ini">
 <pre>    SDTMap <span class="pl-k">userHeader</span> = JCSMPFactory.onlyInstance().createMap()<span class="pl-c"><span class="pl-c">;</span></span>
    try {
      userHeader.putString(<span class="pl-s"><span class="pl-pds">&quot;</span>k_topic<span class="pl-pds">&quot;</span></span>, record.topic())<span class="pl-c"><span class="pl-c">;</span></span>
      userHeader.putInteger(<span class="pl-s"><span class="pl-pds">&quot;</span>k_partition<span class="pl-pds">&quot;</span></span>, record.kafkaPartition())<span class="pl-c"><span class="pl-c">;</span></span>
      userHeader.putLong(<span class="pl-s"><span class="pl-pds">&quot;</span>k_offset<span class="pl-pds">&quot;</span></span>, record.kafkaOffset())<span class="pl-c"><span class="pl-c">;</span></span>
      userHeader.putDestination(<span class="pl-s"><span class="pl-pds">&quot;</span>dynamicDestination<span class="pl-pds">&quot;</span></span>, topic)<span class="pl-c"><span class="pl-c">;</span></span>
    } catch (SDTException e) {
      log.info(<span class="pl-s"><span class="pl-pds">&quot;</span>Received Solace SDTException {}, with the following: {} <span class="pl-pds">&quot;</span></span>, 
          e.getCause(), e.getStackTrace())<span class="pl-c"><span class="pl-c">;</span></span>
    }</pre>
</div> 
<p>In this case the &quot;topic&quot; is the Solace Topic that was created based on data in the Kafka record. Please refer to sample record processor for more details:</p> 
<div class="highlight highlight-source-ini">
 <pre>SolDynamicDestinationRecordProcessor.java</pre>
</div> 
<p>The sample is included with this project.</p> 
<p>It is important to note that if the destination is a Solace Queue, the network topic name for queues can be used. For example, if the queue is &quot;testQueue&quot;, the dynamic topic would be &quot;$P2P/QUE/testQueue&quot;.</p> 
<h4> <a id="message-replay" class="anchor" href="#message-replay" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Message Replay</h4> 
<p>By default, the Solace Sink Connector will start sending Solace events based on the last Kafka Topic offset that was flushed before the connector was stopped. It is possible to use the Solace Sink Connector to replay messages from the Kafka Topic.</p> 
<p>Adding a configuration entry allows the Solace Sink Connector to start processing from an offset position that is different from the last offset that was stored before the connector was stopped. This is controlled by adding the following entry to the connector configuration file:</p> 
<div class="highlight highlight-source-ini">
 <pre><span class="pl-k">sol.kakfa_replay_offset</span>=&lt;offset&gt;</pre>
</div> 
<p>The offset is a Java Long value. A value of 0 will result in the replay of the entire Kafka Topic. A positive value will result in the replay from that offset value for the Kafka Topic. The same offset value will be used against all active partitions for that Kafka Topic.</p> 
<p>To make is easier to determine offset values for the Kafka Topic records, the three Record Processor samples included with this project include the sending of Solace message events that includes the Kafka Topic, Partition and Offset for every Kafka record that corresponds to the specific Solace event message. The Kafka information can be stored in multiple places in the Solace event message without adding the details to the data portion of the event. The three record processing samples add the data to the UserData Solace transport header and the Solace user-specific transport header that is sent as a &quot;User Property Map&quot;.</p> 
<p>A message dump from the Sink Connector generated Solace event messages that is generated using one of the sample Record Processors would be similar to:</p> 
<p><a href="resources/replayDump.png" target="_blank" rel="noopener noreferrer"><img src="resources/replayDump.png" alt="Event Message Dump" style="max-width:100%;" /></a></p> 
<h2> <a id="contributing" class="anchor" href="#contributing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2> 
<p>Please read <a href=".CONTRIBUTING.md.html">CONTRIBUTING.md</a> for details on our code of conduct, and the process for submitting pull requests to us.</p> 
<h2> <a id="authors" class="anchor" href="#authors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Authors</h2> 
<p>See the list of <a href="../../graphs/.contributors.md.html">contributors</a> who participated in this project.</p> 
<h2> <a id="license" class="anchor" href="#license" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>License</h2> 
<p>This project is licensed under the Apache License, Version 2.0. - See the <a href=".LICENSE.md.html">LICENSE</a> file for details.</p> 
<h2> <a id="resources" class="anchor" href="#resources" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Resources</h2> 
<p>For more information about Solace technology in general please visit these resources:</p> 
<ul> 
 <li>The Solace Developer Portal website at: <a href="http://dev.solace.com" rel="nofollow">http://dev.solace.com</a> </li> 
 <li>Understanding <a href="http://dev.solace.com/tech/" rel="nofollow">Solace technology.</a> </li> 
 <li>Ask the <a href="http://dev.solace.com/community/" rel="nofollow">Solace community</a>.</li> 
</ul>
</body>
</html>
